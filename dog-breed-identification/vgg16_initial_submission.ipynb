{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data\n",
    "\n",
    "###### Define root directory for data.\n",
    "This directory should already contain the test.zip and train.zip files from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/gzpjpk/dev/data/dog-breed-identification'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "DATA_PATH = os.environ['DATA'] + '/dog-breed-identification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from vgg16 import Vgg16\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Unzip labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /Users/gzpjpk/dev/fastai/data/dog-breed-identification/labels.csv.zip\r\n",
      "  inflating: /Users/gzpjpk/dev/fastai/data/dog-breed-identification/labels.csv  \r\n"
     ]
    }
   ],
   "source": [
    "!unzip $DATA_PATH/labels.csv.zip -d $DATA_PATH > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Unzip data downloaded from Kaggle into `test/` and `train/` folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!unzip $DATA_PATH/train.zip -d $DATA_PATH > /dev/null\n",
    "!unzip $DATA_PATH/test.zip -d $DATA_PATH > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir    $DATA_PATH/valid\n",
    "!mkdir    $DATA_PATH/results\n",
    "\n",
    "!mkdir -p $DATA_PATH/sample/train\n",
    "!mkdir    $DATA_PATH/sample/test\n",
    "!mkdir    $DATA_PATH/sample/valid\n",
    "!mkdir    $DATA_PATH/sample/results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = glob(DATA_PATH + '/train/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for filepath in shuf[:2000]:\n",
    "    os.rename(filepath, DATA_PATH+'/valid/' + os.path.basename(filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Copy out some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def copyNFromTo(n, src, dest):\n",
    "    g = glob(src + '/*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(n):\n",
    "        filepath = shuf[i]\n",
    "        copyfile(filepath, dest + '/' + os.path.basename(filepath))\n",
    "\n",
    "        \n",
    "copyNFromTo(500, DATA_PATH+'/train', DATA_PATH+'/sample/train')\n",
    "copyNFromTo(100,  DATA_PATH+'/valid', DATA_PATH+'/sample/valid')\n",
    "copyNFromTo(100,  DATA_PATH+'/test',  DATA_PATH+'/sample/test' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Move  images into separate  directories for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "allLabels = { row['breed'] for row in csv.DictReader(open(DATA_PATH + '/labels.csv', 'rb')) }\n",
    "\n",
    "def get_label_for_image(filepath):\n",
    "    name, ext = os.path.splitext(os.path.basename(filepath))\n",
    "    for row in csv.DictReader(open(DATA_PATH+'/labels.csv')):\n",
    "        if name == row['id']:\n",
    "            return row['breed']\n",
    "    raise Exception('Did not find filename match in labels CSV file ' + name)\n",
    "\n",
    "def separateByLabel(dir):\n",
    "    for label in allLabels: os.makedirs(dir + '/' + label)\n",
    "    for filepath in glob(dir + '/*.jpg'):\n",
    "        label = get_label_for_image(filepath)\n",
    "        filename = os.path.basename(filepath)\n",
    "        os.rename(filepath, dir + '/' + label + '/' + filename)\n",
    "\n",
    "separateByLabel(DATA_PATH + '/train')\n",
    "separateByLabel(DATA_PATH + '/valid')\n",
    "separateByLabel(DATA_PATH + '/sample/train')\n",
    "separateByLabel(DATA_PATH + '/sample/valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Move test images into unknown folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moveToUnknown(base_path):\n",
    "    os.makedirs(base_path + '/unknown')\n",
    "    for filepath in glob(base_path+'/*.jpg'):\n",
    "        filename = os.path.basename(filepath)\n",
    "        os.rename(filepath, base_path + '/unknown/' + filename)\n",
    "\n",
    "moveToUnknown(DATA_PATH + '/test')\n",
    "moveToUnknown(DATA_PATH + '/sample/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train VGG16 Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 images belonging to 120 classes.\n",
      "Found 100 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "path = DATA_PATH + '/sample' # use sample data\n",
    "# path = DATA_PATH # use real data\n",
    "\n",
    "vgg = Vgg16()\n",
    "batches = vgg.get_batches(path+'/train', batch_size=64)\n",
    "val_batches = vgg.get_batches(path+'/valid', batch_size=128)\n",
    "vgg.finetune(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latest_weights_filename = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 0\n",
      "Epoch 1/1\n",
      "500/500 [==============================] - 656s - loss: 8.3663 - acc: 0.0100 - val_loss: 5.8567 - val_acc: 0.0300\n",
      "Running epoch: 1\n",
      "Epoch 1/1\n",
      "500/500 [==============================] - 634s - loss: 4.2452 - acc: 0.2000 - val_loss: 5.1559 - val_acc: 0.1000\n",
      "Completed 2 fit operations\n"
     ]
    }
   ],
   "source": [
    "no_of_epochs = 2\n",
    "\n",
    "for epoch in range(no_of_epochs):\n",
    "    print \"Running epoch: %d\" % epoch\n",
    "    vgg.fit(batches, val_batches, nb_epoch=1)\n",
    "    latest_weights_filename = 'ft%d.h5' % epoch\n",
    "    vgg.model.save_weights(DATA_PATH + '/results/' + latest_weights_filename) # saving weights after each epoch\n",
    "print \"Completed %s fit operations\" % no_of_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "batches, preds = vgg.test(path+'/test', batch_size = batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the column ordering (appears that cats are column 1 and dogs are column 2) by viewing some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print preds[:5]\n",
    "print batches.filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open(path + '/test/' + batches.filenames[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save test results arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path + '/results/test_preds.dat', preds)\n",
    "save_arrayray(path + '/results/filenames.dat', batches.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.model.load_weights(DATA_PATH + '/results/ft1_full.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "val_batches, probs = vgg.test(DATA_PATH + '/valid', batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00ca18751837cd6a22813f8e221f7819' '11b60d8d86f14a601ca290909a17cbc6'\n",
      " '19de1db12b3ddc7f2af6d9453c977083' ..., '9afd17ba252823662440863d6c0e'\n",
      " 'b9b54494a2ed02ea74f0ef26a8cc' 'c2c60183f18666aaa714efeff54a']\n"
     ]
    }
   ],
   "source": [
    "expected_labels = val_batches.classes \n",
    "\n",
    "ids = np.array([ f[f.find('/')+1:testname.find('.')] for f in val_batches.filenames ])\n",
    "\n",
    "\n",
    "##Round our predictions to 0/1 to generate labels\n",
    "## our_predictions = probs[:,0]\n",
    "## our_labels = np.round(1-our_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.preprocessing import image\n",
    "#\n",
    "##Helper function to plot images by index in the validation set \n",
    "##Plots is a helper function in utils.py\n",
    "#def plots_idx(idx, titles=None):\n",
    "#    plots([image.load_img(DATA_PATH + '/valid/' + val_batches.filenames[i]) for i in idx], titles=titles)\n",
    "#    \n",
    "##Number of images to view for each visualization task\n",
    "#n_view = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #1. A few correct labels at random\n",
    "# correct = np.where(our_labels==expected_labels)[0]\n",
    "# print \"Found %d correct labels\" % len(correct)\n",
    "# idx = permutation(correct)[:n_view]\n",
    "# plots_idx(idx, our_predictions[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract imageIds from the filenames in our test/unknown directory \n",
    "filenames = load_array(DATA_PATH + '/results/filenames.dat')\n",
    "# ids = np.array([ f[f.find('/')+1:testname.find('.')] for f in batches.filenames ])\n",
    "ids = np.array([ f[f.find('/')+1:f.find('.')] for f in filenames ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the columns into an array of [id, prob1, prob2, ... , prob120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10357, 121)\n",
      "/Users/gzpjpk/dev/data/dog-breed-identification/results/submission.csv\n"
     ]
    }
   ],
   "source": [
    "pred_percents = load_array(DATA_PATH + '/results/test_preds.dat')\n",
    "reshaped_ids = np.reshape(ids, (-1,1)) # convert from 1d to 2d\n",
    "subm = np.append(reshaped_ids, pred_percents, axis=1)\n",
    "\n",
    "f = open(DATA_PATH + '/sample_submission.csv')\n",
    "header = f.readline().strip()\n",
    "\n",
    "submission_file_path = DATA_PATH + '/results/submission.csv'\n",
    "format = ['%s' for _ in range(121)]\n",
    "print subm.shape\n",
    "np.savetxt(submission_file_path, subm, fmt=format, delimiter=',', header=header, comments='')\n",
    "print submission_file_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
